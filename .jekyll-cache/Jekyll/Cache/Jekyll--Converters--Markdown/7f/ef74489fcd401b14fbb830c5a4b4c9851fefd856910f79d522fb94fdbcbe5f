I";%<style type="text/css">
code.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}
</style>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry
    }
});
MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<h2 id="probability-and-statistics-review">Probability and Statistics Review</h2>

<p>The topic on probability and statistics is seemingly off for an optimization class, but in fact it is a closely 
related topic. There are several reasons to pick up basic probability and statistics background before 
we move forward in the class.</p>

<ol>
  <li>
    <p><strong>Response surface</strong>: The evaluation of engineering systems often requires significant computational cost, 
which makes optimization algorithm unafforable or even intractable. In these case, response surface methods 
can help. A response surface is a <strong>statistical</strong> model that approximates the true physical responses of 
the system to be designed, allowing fast prediction of system performance
without running the actual simulations or experiments.</p>
  </li>
  <li>
    <p><strong>Parameter estimation</strong>: Some systems under design may contain unknown parameters, e.g., material 
elasticity or battery resistance. To be able to optimize such systems, we need to first identify these 
parameters. Parameter estimation is essentially a regression problem.</p>
  </li>
  <li>
    <p><strong>Reliability based design optimization</strong>: Some design optimization problems have probabilistic 
objective, e.g., the profit of selling a product, or fatigue of a material, due to uncontrollable 
factors, e.g., consumer choices, or material initial cracks. In these 
cases, it is important that we not only design for good average performance, but also for minimal 
chance of failure. For example, one can define the objective as \(F(x) = p(f(x)&gt;f^*)\) where 
\(f^*\) is a lower bound. To optimize for such an objective, one will
need to quantify the probabilistic distribution of \(f(x)\).</p>
  </li>
</ol>

<h3 id="probability-vs-statistics">Probability vs. statistics</h3>

<p><strong>Probability</strong> theory is about the derivation and analysis of 
<strong>models</strong> that may help to explain real-world observations through deduction. E.g., we can deduct the 
probability of having two students with the same birthday 
in the class without using data.</p>

<p><strong>Statistics</strong> is about the analysis of <strong>data</strong> that may help to derive hypotheses
and models through induction.
E.g., we can test the hypothesis that people born more often in the fall (I made it up) if we
observe such a trend from the data.</p>

<h3 id="continuous-random-variables">Continuous random variables</h3>

<p>Probability density function (pdf) \(f_X(x)\) of random variable \(X\) describes the probability:</p>

\[P(x_1 \leq X \leq x_2) = \int_{x_1}^{x_2} f_X(x)dx\]

<p>pdf properties: \(f_X(x) \geq 0, \qquad \int_{-\infty}^{\infty} f_X(x) = 1\)</p>

<p>Let \(X\) be a continuous random variable with pdf \(f_X(x)\). The <strong>mean</strong> (expected value) 
of \(X\) is \(\mu = \mathbb{E}[X] = \int_{-\infty}^{\infty} x f_X(x) dx\).</p>

<p>The variance is \(\sigma^2 = \int_{-\infty}^{\infty} (x-\mu)^2 f_X(x) dx\).</p>

<h3 id="normal-distribution">Normal distribution</h3>

<p>A normal distribution has pdf</p>

\[f_X(x) = \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}\]

<p>When $\mu = 0$ and $\sigma = 1$, we have a <strong>standard</strong> normal distribution.</p>

<h3 id="how-to-derive-the-normal-pdf">How to derive the normal pdf?</h3>

<p>Consider throwing a dart at the origin of an x-y plane. You are aiming at the origin, but random errors 
in your throw will produce varying results. We assume that:</p>

<ul>
  <li>errors in x and y directions are independent</li>
  <li>chance to hit anywhere on a circle is the same</li>
  <li>large errors are less likely than small errors</li>
</ul>

<p>See <a href="http://www.ncssm.edu/courses/math/Talks/PDFS/normal.pdf">the derivation of normal pdf</a></p>

<h3 id="probability-calculation-under-normal-pdf">Probability calculation under normal pdf</h3>

<p>For a general normal distribution random variable \(X \sim N(\mu, \sigma^2)\), the probability for \(X\) to assume a 
value between \(x_1\) and \(x_2\) can be calculated by using definition:</p>

\[P(x_1 \leq X \leq x_2) = \int_{x_1}^{x_2} \frac{1}{\sigma\sqrt{2\pi}} e^{\frac{(x-\mu)^2}{2\sigma^2}}dx\]

<p>This integral does not have a closed-form solution.</p>

<h3 id="cumulative-distribution-function-cdf">Cumulative distribution function (cdf)</h3>

<p>The cdf for a random variable \(X\) is</p>

<p>\(F_X(x) = \int_{-\infty}^{x} f_X(x)dx\).</p>

<p>Therefore
\(P(x_1 \leq X \leq x_2) = F_X(x_2) - F_X(x_1)\)</p>

<h3 id="transformation-to-standard-normal">Transformation to standard normal</h3>

<p>A general normal random variable \(X \sim N(\mu, \sigma^2)\) can be transformed in to a 
standard normal random variable \(Z\) by</p>

\[Z = \frac{X-\mu}{\sigma}\]

<p>Probability calculation
\(P(x_1 \leq X \leq x_2) = P(\frac{x_1-\mu}{\sigma}\leq \frac{X-\mu}{\sigma} \leq \frac{x_2-\mu}{\sigma}) = P(z_1 \leq Z \leq z_2)\).</p>

<h3 id="exercise-1">Exercise 1</h3>

<p>A certain type of storage battery lasts, on average, 3 years with a standard deviation of 0.5 years. 
Assuming that the battery life are normally distributed.</p>

<p>Determine the probability that a given battery will last more than 2.3 years.</p>

<p>Determine the probability that a given battery will last more than 2 but less than 3.5 years.</p>

<h3 id="independently-and-identically-distributed-random-variables">Independently and identically distributed random variables</h3>

<p>Let repeated measurements \(x_1, x_2, \cdots, x_n\) be drawn from the same distribution. 
We can consider these measurements as realizations of \(n\) <strong>identically and independently distributed</strong> (iid) random variables:</p>

<p>\(X_1, \cdots, X_n \sim f_X(x)\),</p>

<p>with mean \(\mu\) and variance \(\sigma^2\).</p>

<p>Let \(\bar{X} = \frac{1}{n}\sum_{i=1}^n X_i\) be the average of these measurements. The mean of \(\bar{X}\) is</p>

<p>\(\mu_{\bar{X}} = E(\frac{1}{n}\sum_{i=1}^n X_i) = \mu\).</p>

<p>The variance is</p>

<p>\(\sigma^2_{\bar{X}} = E\left((\frac{1}{n}\sum_{i=1}^n X_i - \mu)^2\right) = \frac{\sigma^2}{n}\).</p>

<p>Therefore, the average of normal random variables is a random variable:</p>

<p>\(\bar{X} \sim N(\mu, \frac{\sigma^2}{n})\).</p>

<h3 id="exercise-2">Exercise 2</h3>

<p>A voltage measurement \(X\) has a normal distribution with \(\mu\) = 40 (V) and \(\sigma\) = 6 (V).</p>

<p>Find the value of \(x\) such that \(P(X \leq x) = 45\%\)</p>

<p>Find the value of \(x\) such that \(P(X \geq x) = 14\%\)</p>

<p>Find the value of \(d_1\) such that \(P(\mu - d_1 \leq X \leq \mu + d_1) = 90\%\)</p>

<p>If 3 measurements are made and averaged, find the value of \(d_2\) such that \(P(\mu - d_2 \leq X_{avg} \leq \mu + d_2) = 90\%\)</p>

<h3 id="likelihood">Likelihood</h3>

<p>Consider a pdf \(p(x;\theta)\) where \(x\) is the random variable and \(\theta\) is the parameter of the pdf. E.g., when 
the pdf is a normal distribution, \(x \sim p(x;\theta)\) where \(\theta\) represents the mean and the standard deviation.</p>

<p>If we are given data \(x_1\), \(x_2\), …,\(x_n\). Can we <strong>estimate</strong> what \(\theta\) is?</p>

<p>Parameter estimation relies on the concept of <strong>likeliood</strong>.</p>

<p>Here consider that \(x_1\), \(x_2\), …,\(x_n\) are iid samples from \(p(x;\theta)\). Then the likelihood function is</p>

<p>\(L(\theta; x_1,\cdots,x_n) = \prod_{i=1}^n p(x_i;\theta)\).</p>

<p>Or more often, we use the <strong>negative log-likelihood</strong> function:</p>

<p>\(l(\theta; x_1,\cdots,x_n) = -\sum_{i=1}^n \log p(x_i;\theta)\).</p>

<p>The <strong>maximum likelihood estimations</strong> (MLE) are derived by minimizing \(l\) with respect to \(\theta\). Take normal
distribution as an example, we have</p>

\[l(\theta; x_1,\cdots,x_n) = \sum_{i=1}^n (\frac{(x_i-\mu)^2}{2\sigma^2} + \log \sqrt{2\pi} + \log\sigma)\]

<p>The optimal solutions are \(\mu_{MLE} = \frac{\sum_{i=1}^n x_i}{n}\), and \(\sigma_{MLE}^2 = \frac{\sum_{i=1}^n (x_i-\mu)^2}{n}\).</p>

<p>Notice that both \(\mu_{MLE}\) and \(\sigma_{MLE}^2\) are random variables since \(x_i\) are random variables.</p>

<p>The mean of \(\mu^*\) is \(\mu\), thus we call \(\mu^*\) an <strong>unbiased</strong> estimate.</p>

<p>For \(\sigma_{MLE} = \frac{\sum_{i=1}^n (x_i-\mu)^2}{n}\), since the calculation involves \(\mu\) which is unknown, one can 
plug in \(\mu_{MLE}\) as an approximation. However, doing so will make \(\sigma_{MLE}\) a biased estimate. One can check by calculating
the mean of \(\sigma_{MLE}^2\)</p>

<p>\(\mathbb{E}[\sigma_{MLE}^2] = \mathbb{E}[\frac{\sum_{i=1}^n (x_i-\mu)^2}{n}] = \frac{n-1}{n} \sigma^2\).</p>

<p>Therefore, we use the <a href="https://en.wikipedia.org/wiki/Bessel%27s_correction">correction</a> \(\sigma_{MLE}^2 = \frac{\sum_{i=1}^n (x_i-\mu_{MLE})^2}{n-1}\).</p>

:ET